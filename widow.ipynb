{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file widiow.txt and read it \n",
    "with open('widow.txt', 'r') as file:\n",
    "    text = file.read().replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedText = [] \n",
    "for sent in sentences: \n",
    "    tokens = re.sub(\"[^a-z0-9]+\",\" \",sent.lower())\n",
    "    normalizedText.append(tokens)\n",
    "result = [ word_tokenize(s) for s in normalizedText ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=result , vector_size = 180 , window = 4, min_count=2 , workers = 4 , sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('women', 0.2551659345626831),\n",
       " ('burden', 0.24160641431808472),\n",
       " ('against', 0.23628844320774078),\n",
       " ('spouses', 0.20751942694187164),\n",
       " ('the', 0.20169739425182343),\n",
       " ('not', 0.1904909461736679),\n",
       " ('by', 0.19035615026950836),\n",
       " ('another', 0.18514233827590942),\n",
       " ('be', 0.18389874696731567),\n",
       " ('of', 0.18334296345710754)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('widow', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('widow2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = KeyedVectors.load_word2vec_format('widow2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 크기(shape) : (145, 180)\n"
     ]
    }
   ],
   "source": [
    "print('모델의 크기(shape) :',loaded_model.vectors.shape)\n",
    "# 145의 차원을 가진 word2vec 벡터가 180개 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'The state of having lost ones spouse to death is termed widowhood',\n",
    "    'a woman who has lost her spouse by death and has not remarried.',\n",
    "    'widow is burden',\n",
    "    'spouses means persons wife',\n",
    "    'spouses means a husband or wife, considered in relation to their partner.',\n",
    "    'My today has been really tough, but I am seeing it as another chance to focus on healing and moving forward.',\n",
    "    'I am ending with a few chapters on a book and some journaling to process my thoughts.',\n",
    "    'Throughout the day I found a few moments to check in with my friends and family, knowing that their love and company are essential for my healing process.',\n",
    "]\n",
    "y_train = [1,1,1,1,1,1,1,1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : 76\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
    "print('단어 집합 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 결과 : [[7, 23, 24, 25, 8, 26, 9, 1, 10, 11, 27, 28], [2, 29, 30, 5, 8, 31, 9, 32, 10, 3, 5, 33, 34], [35, 11, 36], [12, 13, 37, 14], [12, 13, 2, 38, 39, 14, 40, 15, 41, 1, 16, 42], [4, 43, 5, 44, 45, 46, 47, 6, 17, 48, 49, 50, 51, 52, 1, 53, 18, 19, 3, 54, 55], [6, 17, 56, 20, 2, 21, 57, 18, 2, 58, 3, 59, 60, 1, 22, 4, 61], [62, 7, 63, 6, 64, 2, 21, 65, 1, 66, 15, 20, 4, 67, 3, 68, 69, 70, 16, 71, 3, 72, 73, 74, 75, 4, 19, 22]]\n",
      "최대 길이 : 28\n",
      "패딩 결과 :\n",
      "[[ 7 23 24 25  8 26  9  1 10 11 27 28  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [ 2 29 30  5  8 31  9 32 10  3  5 33 34  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [35 11 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [12 13 37 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [12 13  2 38 39 14 40 15 41  1 16 42  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [ 4 43  5 44 45 46 47  6 17 48 49 50 51 52  1 53 18 19  3 54 55  0  0  0\n",
      "   0  0  0  0]\n",
      " [ 6 17 56 20  2 21 57 18  2 58  3 59 60  1 22  4 61  0  0  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [62  7 63  6 64  2 21 65  1 66 15 20  4 67  3 68 69 70 16 71  3 72 73 74\n",
      "  75  4 19 22]]\n",
      "[1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print('정수 인코딩 결과 :',X_encoded)\n",
    "max_len = max(len(l) for l in X_encoded)\n",
    "print('최대 길이 :',max_len)\n",
    "X_train = pad_sequences(X_encoded, maxlen= max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print('패딩 결과 :')\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 180))\n",
    "def get_vector(word):\n",
    "    if word in loaded_model:\n",
    "        return loaded_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = get_vector(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5543014e-03  5.0518531e-03  2.6298468e-03 -2.2227594e-03\n",
      " -9.3628105e-04 -1.4578261e-03  4.1421140e-03  3.0400320e-03\n",
      " -4.8831264e-03  1.5014106e-03  3.4770013e-03 -1.1652287e-03\n",
      " -1.6344019e-03  2.2051195e-03 -2.9978456e-03 -1.2554310e-03\n",
      "  3.2921962e-03 -3.5982742e-03 -6.8841311e-03  8.2824141e-04\n",
      " -3.1833842e-03 -3.5017198e-03  5.9437947e-03  3.0079756e-03\n",
      "  5.2983882e-03 -7.1264724e-03 -1.2695376e-03 -4.5329980e-03\n",
      "  1.7685987e-04 -5.2175559e-03  4.4770483e-03  8.2440639e-04\n",
      " -6.4092450e-04 -5.5728399e-04  6.2878910e-03 -2.2451553e-05\n",
      " -2.0939508e-03 -4.6842378e-03  2.0122917e-03  6.1489139e-03\n",
      "  2.4584220e-03 -2.4827456e-03  1.1989875e-03 -1.2798873e-03\n",
      " -5.5495868e-03 -1.8700454e-03  3.4627496e-04  2.9323173e-03\n",
      " -2.9523307e-03  1.8967528e-04 -2.8107308e-03 -8.6703961e-04\n",
      " -9.4280578e-04  3.4729799e-04 -6.0255639e-04 -3.5824403e-03\n",
      "  1.5913261e-03 -5.8365376e-03  3.8421177e-03 -6.0166125e-03\n",
      " -6.1211004e-03 -1.9964369e-03  2.8728852e-03  1.0633443e-03\n",
      " -4.6137138e-03 -3.7857180e-03  3.9487490e-03  7.4362443e-03\n",
      "  1.2613189e-03 -3.4667044e-03  6.0889721e-03  1.7164289e-03\n",
      "  2.6809261e-03  3.6828346e-03 -2.7598322e-03  2.2718788e-03\n",
      "  3.7274091e-03 -2.4562685e-03  2.2725139e-03  5.4335431e-03\n",
      "  1.8390841e-03 -3.8423808e-03  4.0244069e-03  1.9481907e-03\n",
      "  4.0545240e-03  3.6499212e-03 -1.7809873e-03  7.1791647e-04\n",
      "  5.8839084e-03  4.1982904e-03  4.8796297e-04  2.1084233e-03\n",
      "  2.2101721e-03  2.4114887e-03 -3.1259512e-03  5.3913356e-03\n",
      " -6.7351945e-03  2.8846457e-03 -1.2490279e-03 -3.0512998e-03\n",
      " -4.4610878e-03 -1.6661389e-03  9.8403986e-04 -3.8145636e-03\n",
      " -5.3581763e-03  3.1829965e-03 -3.3802153e-03 -4.0479680e-03\n",
      " -1.3330126e-04  4.8786709e-03  5.9085521e-03  4.1529881e-03\n",
      "  5.9159650e-03 -5.9029681e-04  2.9836176e-03  2.1039932e-03\n",
      "  6.2481938e-03  2.0139264e-03  6.7951437e-03 -5.7060532e-03\n",
      " -4.2952169e-03 -3.2240713e-03 -2.0166328e-03 -1.7828140e-03\n",
      "  1.8140333e-03  5.1917969e-03 -2.1515721e-03 -2.0207479e-03\n",
      "  4.6132500e-03  1.6798435e-03 -3.8871784e-03  4.1757990e-03\n",
      "  4.9634199e-03  6.2518299e-04 -2.2049779e-03 -7.3764450e-03\n",
      "  3.9404188e-03 -4.2529695e-04  4.2930787e-04 -3.2217354e-03\n",
      "  4.3368815e-03 -3.2483235e-03 -1.4556966e-04  3.1617819e-03\n",
      "  9.1860775e-04  1.5369420e-03 -5.5770776e-03 -1.8291632e-03\n",
      "  5.6367009e-03  1.7039434e-03  3.8153746e-03  9.0462528e-04\n",
      " -3.9672209e-03  3.9071748e-03  5.4625897e-03  4.5766644e-03\n",
      "  4.3057180e-03 -1.9418226e-03 -5.7581104e-03  5.7179015e-03\n",
      "  5.3133466e-03  3.2425201e-03 -3.6261063e-03  4.3642754e-03\n",
      " -5.9521891e-04 -5.0664255e-03 -2.2831606e-05 -1.4101403e-03\n",
      " -1.3550336e-03  2.1966854e-03  4.4041909e-03  3.3343190e-03\n",
      " -4.6844566e-03 -1.6115891e-03 -4.3244953e-03 -2.1050319e-03\n",
      " -3.8636448e-03 -2.3798761e-03 -5.1623106e-04 -6.4203609e-04]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 women의 맵핑된 정수 : 29\n"
     ]
    }
   ],
   "source": [
    "print('단어 women의 맵핑된 정수 :', tokenizer.word_index['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 woman의 임베딩 벡터값:  [-2.55430141e-03  5.05185314e-03  2.62984680e-03 -2.22275942e-03\n",
      " -9.36281052e-04 -1.45782612e-03  4.14211396e-03  3.04003200e-03\n",
      " -4.88312636e-03  1.50141062e-03  3.47700133e-03 -1.16522866e-03\n",
      " -1.63440185e-03  2.20511947e-03 -2.99784564e-03 -1.25543098e-03\n",
      "  3.29219620e-03 -3.59827420e-03 -6.88413111e-03  8.28241406e-04\n",
      " -3.18338419e-03 -3.50171980e-03  5.94379473e-03  3.00797564e-03\n",
      "  5.29838819e-03 -7.12647242e-03 -1.26953761e-03 -4.53299796e-03\n",
      "  1.76859874e-04 -5.21755591e-03  4.47704829e-03  8.24406394e-04\n",
      " -6.40924496e-04 -5.57283987e-04  6.28789095e-03 -2.24515534e-05\n",
      " -2.09395075e-03 -4.68423776e-03  2.01229169e-03  6.14891388e-03\n",
      "  2.45842198e-03 -2.48274556e-03  1.19898748e-03 -1.27988728e-03\n",
      " -5.54958684e-03 -1.87004544e-03  3.46274959e-04  2.93231732e-03\n",
      " -2.95233075e-03  1.89675280e-04 -2.81073083e-03 -8.67039606e-04\n",
      " -9.42805782e-04  3.47297988e-04 -6.02556393e-04 -3.58244032e-03\n",
      "  1.59132609e-03 -5.83653757e-03  3.84211773e-03 -6.01661252e-03\n",
      " -6.12110039e-03 -1.99643685e-03  2.87288520e-03  1.06334430e-03\n",
      " -4.61371383e-03 -3.78571800e-03  3.94874904e-03  7.43624428e-03\n",
      "  1.26131892e-03 -3.46670439e-03  6.08897209e-03  1.71642890e-03\n",
      "  2.68092612e-03  3.68283456e-03 -2.75983219e-03  2.27187877e-03\n",
      "  3.72740906e-03 -2.45626853e-03  2.27251393e-03  5.43354312e-03\n",
      "  1.83908409e-03 -3.84238083e-03  4.02440690e-03  1.94819074e-03\n",
      "  4.05452400e-03  3.64992116e-03 -1.78098725e-03  7.17916468e-04\n",
      "  5.88390836e-03  4.19829041e-03  4.87962971e-04  2.10842327e-03\n",
      "  2.21017213e-03  2.41148868e-03 -3.12595116e-03  5.39133558e-03\n",
      " -6.73519447e-03  2.88464571e-03 -1.24902790e-03 -3.05129983e-03\n",
      " -4.46108775e-03 -1.66613888e-03  9.84039856e-04 -3.81456362e-03\n",
      " -5.35817631e-03  3.18299653e-03 -3.38021526e-03 -4.04796796e-03\n",
      " -1.33301262e-04  4.87867091e-03  5.90855209e-03  4.15298808e-03\n",
      "  5.91596495e-03 -5.90296811e-04  2.98361760e-03  2.10399320e-03\n",
      "  6.24819379e-03  2.01392639e-03  6.79514371e-03 -5.70605323e-03\n",
      " -4.29521687e-03 -3.22407135e-03 -2.01663282e-03 -1.78281404e-03\n",
      "  1.81403325e-03  5.19179692e-03 -2.15157215e-03 -2.02074787e-03\n",
      "  4.61325003e-03  1.67984352e-03 -3.88717838e-03  4.17579897e-03\n",
      "  4.96341987e-03  6.25182991e-04 -2.20497791e-03 -7.37644499e-03\n",
      "  3.94041883e-03 -4.25296952e-04  4.29307867e-04 -3.22173536e-03\n",
      "  4.33688145e-03 -3.24832345e-03 -1.45569662e-04  3.16178193e-03\n",
      "  9.18607751e-04  1.53694197e-03 -5.57707762e-03 -1.82916317e-03\n",
      "  5.63670089e-03  1.70394336e-03  3.81537457e-03  9.04625282e-04\n",
      " -3.96722089e-03  3.90717480e-03  5.46258967e-03  4.57666442e-03\n",
      "  4.30571800e-03 -1.94182259e-03 -5.75811043e-03  5.71790151e-03\n",
      "  5.31334663e-03  3.24252015e-03 -3.62610631e-03  4.36427537e-03\n",
      " -5.95218909e-04 -5.06642554e-03 -2.28316057e-05 -1.41014031e-03\n",
      " -1.35503360e-03  2.19668541e-03  4.40419093e-03  3.33431899e-03\n",
      " -4.68445662e-03 -1.61158910e-03 -4.32449533e-03 -2.10503186e-03\n",
      " -3.86364479e-03 -2.37987610e-03 -5.16231055e-04 -6.42036088e-04]\n"
     ]
    }
   ],
   "source": [
    "print('단어 woman의 임베딩 벡터값: ', embedding_matrix[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.6930 - acc: 0.3750 - 323ms/epoch - 323ms/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6694 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6451 - acc: 1.0000 - 16ms/epoch - 16ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6191 - acc: 1.0000 - 17ms/epoch - 17ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.5913 - acc: 1.0000 - 22ms/epoch - 22ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.5617 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.5303 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.4973 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.4631 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.4279 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.3923 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.3566 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.3214 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.2871 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.2541 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.2229 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.1939 - acc: 1.0000 - 18ms/epoch - 18ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.1673 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.1432 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.1218 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.1030 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.0867 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.0728 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.0609 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.0510 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.0427 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.0358 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.0302 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.0255 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.0216 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.0184 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.0158 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.0136 - acc: 1.0000 - 16ms/epoch - 16ms/step\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.0118 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.0103 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.0091 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.0080 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.0072 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.0064 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.0058 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.0052 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.0048 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.0044 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.0040 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.0037 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.0035 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.0032 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.0030 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.0029 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.0027 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.0026 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.0024 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.0023 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.0022 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.0021 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.0020 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.0020 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.0019 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.0018 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.0018 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.0017 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.0017 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.0016 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.0016 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.0015 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.0015 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.0015 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.0014 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.0014 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.0014 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.0014 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.0013 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.0013 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.0013 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.0013 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.0012 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.0012 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.0012 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.0012 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.0012 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.0012 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 15ms/epoch - 15ms/step\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.0011 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.0010 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.0010 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.0010 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.0010 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 9.9896e-04 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 9.8772e-04 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 9.7672e-04 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 9.6597e-04 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 9.5543e-04 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 9.4512e-04 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 9.3501e-04 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 9.2509e-04 - acc: 1.0000 - 8ms/epoch - 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9907cfeb0>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 180, weights=[embedding_matrix], input_length=max_len)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
